# 网站追踪器

这个脚本用于追踪和记录指定网站的重定向情况，特别设计用于监控不断更新的内容。

## 功能

- 访问指定的URL范围（如 shizu1 到 shizu50）
- 记录每个URL重定向到的最终目标
- 添加时间戳和标识
- 记录部分页面的标题
- 使用随机延迟避免触发反爬机制
- **智能反爬机制检测**：检测到反爬机制时自动延长等待时间
- **断点续传**：可以从上次中断的地方继续
- **错误重试**：自动重试失败的请求
- **生成统计报告**：提供爬取结果的统计分析

## 使用方法

1. 首先运行 `install_dependencies.bat` 安装所需的Python依赖
2. 根据需要编辑 `config.json` 文件调整参数：
   - `base_url`: 基础URL前缀
   - `min_index` 和 `max_index`: 要访问的URL编号范围
   - `min_delay` 和 `max_delay`: 随机延迟的秒数范围
   - `results_file`: 结果保存的文件名
   - `log_file`: 日志文件名
   - `title_sample_rate`: 获取标题的采样率（例如：5表示每5个链接获取一次标题）
   - `max_retries`: 每个链接最大重试次数
   - `retry_delay`: 重试前的等待时间(秒)
   - `anti_crawl_wait_min` 和 `anti_crawl_wait_max`: 触发反爬后的等待时间范围(秒)
3. 运行 `run_tracker.bat` 开始追踪
4. 如需生成报告，运行 `generate_report.bat`

## 断点续传功能

- 程序会自动在 `tracker_progress.json` 文件中记录进度
- 如果程序中断（如按Ctrl+C或断电），下次运行时会从上次的位置继续
- 可以通过删除进度文件强制从头开始

## 反爬机制处理

- 如果检测到重定向到腾讯视频页面 (`v.qq.com/txp/iframe/player.html`)，表明触发了反爬机制
- 脚本会自动延长等待时间，并随着连续失败次数增加等待时间
- 在日志和结果文件中会标记这种情况

## 注意事项

- 程序会自动处理错误并继续运行
- 结果保存在CSV格式文件中，可以用Excel打开
- 避免过于频繁地运行脚本，以防被网站封禁IP
- 如遇持续触发反爬机制，建议更换IP或增加等待时间

## 输出格式

输出文件包含以下列：
- 时间戳
- 链接编号
- 原始URL
- 跳转URL
- 页面标题（采样）
- 状态（成功/失败/需要重试/出错/跳过）
